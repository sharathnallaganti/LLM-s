{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d917a8f8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001688,
     "end_time": "2025-09-22T04:02:06.346556",
     "exception": false,
     "start_time": "2025-09-22T04:02:06.344868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "# LLM Fine-Tuning & Adaptation\n",
    "\n",
    "Fine-tuning is about **taking a pretrained large language model (LLM)** and adapting it to new tasks, domains, or behaviors **without training from scratch**. This reduces cost, data needs, and compute compared to pretraining.\n",
    "\n",
    "\n",
    "\n",
    "## 1. **Why Fine-Tuning?**\n",
    "\n",
    "* Pretrained LLMs are **generalists** → trained on massive, diverse text.\n",
    "* Real-world use cases need **specialists** → e.g., legal assistant, medical chatbot, code generator.\n",
    "* Fine-tuning narrows the model’s behavior to specific tasks or improves alignment.\n",
    "\n",
    "\n",
    "\n",
    "## 2. **Types of Fine-Tuning & Adaptation**\n",
    "\n",
    "###  (a) **Supervised Fine-Tuning (SFT)**\n",
    "\n",
    "* Train the LLM on **task-specific labeled data** (input → desired output).\n",
    "* Example: Fine-tuning GPT-like model on customer support transcripts.\n",
    "* Pros: Strong task alignment.\n",
    "* Cons: Needs high-quality labeled data.\n",
    "\n",
    "\n",
    "###  (b) **Instruction Tuning**\n",
    "\n",
    "* Extend SFT by training on **instruction–response pairs**.\n",
    "* Helps LLMs better follow human prompts.\n",
    "* Example: FLAN-T5, InstructGPT.\n",
    "* Effect: Improves general usability across many instructions, not just one task.\n",
    "\n",
    "\n",
    "\n",
    "###  (c) **RLHF (Reinforcement Learning from Human Feedback)**\n",
    "\n",
    "* **Stage 1:** Pretrain LLM → SFT with instructions.\n",
    "* **Stage 2:** Train a **reward model** (predicts quality of responses based on human preference).\n",
    "* **Stage 3:** Optimize LLM with reinforcement learning (PPO, DPO, etc.).\n",
    "* Effect: Makes LLMs more **helpful, harmless, honest**.\n",
    "* Example: OpenAI’s ChatGPT, Anthropic’s Claude.\n",
    "\n",
    "\n",
    "\n",
    "### (d) **Parameter-Efficient Fine-Tuning (PEFT)**\n",
    "\n",
    "Instead of retraining all parameters, **update only a small subset**:\n",
    "\n",
    "* **LoRA (Low-Rank Adaptation):** Inserts low-rank matrices into attention layers → huge savings in compute & storage.\n",
    "* **Adapters:** Add small bottleneck layers between frozen transformer layers.\n",
    "* **Prefix / Prompt Tuning:** Train a set of “soft prompts” (learned embeddings) while keeping the model frozen.\n",
    "* Pros: Cheap, efficient, can maintain multiple domain experts.\n",
    "* Cons: May underperform full fine-tuning on complex tasks.\n",
    "\n",
    "\n",
    "###  (e) **Domain Adaptation**\n",
    "\n",
    "* Fine-tuning with **unlabeled domain-specific corpora** (continued pretraining).\n",
    "* Example: Biomedical text → BioBERT, Legal text → Legal-BERT.\n",
    "* Effect: Improves vocabulary & domain knowledge.\n",
    "\n",
    "\n",
    "\n",
    "###  (f) **Multi-Task & Mixture-of-Experts**\n",
    "\n",
    "* Fine-tuning on **multiple related tasks** (summarization, QA, translation).\n",
    "* Mixture-of-experts routes queries to specialized subnetworks.\n",
    "\n",
    "\n",
    "\n",
    "## 3. **Practical Workflow**\n",
    "\n",
    "1. **Select base model** → GPT, LLaMA, Falcon, Mistral, etc.\n",
    "2. **Prepare data** → task-specific or instruction-style, cleaned & formatted.\n",
    "3. **Choose fine-tuning strategy**:\n",
    "\n",
    "   * SFT if you have high-quality labels.\n",
    "   * Instruction tuning for usability.\n",
    "   * RLHF for alignment & safety.\n",
    "   * LoRA/adapters if compute-limited.\n",
    "4. **Train & evaluate** → validate on held-out dataset.\n",
    "5. **Deploy as API / service**.\n",
    "\n",
    "\n",
    "\n",
    "## 4. **Challenges & Tradeoffs**\n",
    "\n",
    "* **Data quality**: Garbage in → garbage out.\n",
    "* **Catastrophic forgetting**: Over-tuning may erase general knowledge.\n",
    "* **Compute cost**: Full fine-tuning = very expensive; PEFT helps.\n",
    "* **Ethical concerns**: Biases may amplify if domain data is skewed.\n",
    "* **Model drift**: Fine-tuned models may become outdated quickly.\n",
    "\n",
    "\n",
    "## 5. **Real-World Examples**\n",
    "\n",
    "* **ChatGPT (OpenAI)** → GPT + SFT + RLHF.\n",
    "* **BloomZ** → BLOOM + Instruction tuning across 46 languages.\n",
    "* **BioGPT, PubMedBERT** → domain adaptation for biomedical research.\n",
    "* **Alpaca / Vicuna** → LLaMA + Instruction fine-tuning with smaller datasets.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4.864919,
   "end_time": "2025-09-22T04:02:06.566347",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-22T04:02:01.701428",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
