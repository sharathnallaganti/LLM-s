{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n# Core Training Terms in LLMs\n\n## 1. **Token**\n\n* **Definition**: The smallest unit of text the model sees (word, subword, or character, depending on the tokenizer).\n* Example: `\"The cat sat.\" → [“The”, “cat”, “sat”, “.”]` (4 tokens).\n* **Analogy**: Like breaking your book into Lego bricks.\n\n\n\n## 2. **Sequence length (context window)**\n\n* **Definition**: The maximum number of tokens the model can process at once.\n* Example: If `sequence_length = 512`, the model can only “see” 512 tokens from your book at a time.\n* **Analogy**: Like a human who can only read **one page at a time** even though the book has 1000 pages.\n\n\n\n## 3. **Stride**\n\n* **Definition**: How far you move forward when making new training sequences from the text.\n* Example:\n\n  * Sequence 1: tokens 1 → 512\n  * With stride = 256 → Sequence 2 starts at token 257 → 768\n  * This way, sequences **overlap**, so the model learns connections across page breaks.\n* **Analogy**: Like sliding a magnifying glass over text — if you move it halfway each time, you still see overlapping context.\n\n\n\n## 4. **Batch**\n\n* **Definition**: A group of sequences processed together in **one forward + backward pass**.\n* Example: If `batch_size = 8` and `sequence_length = 512`, each batch has `8 × 512 = 4096 tokens`.\n* **Analogy**: Like testing yourself with 8 flashcards at the same time, instead of just 1.\n\n\n\n## 5. **Mini-batch vs Batch**\n\n* In practice, “batch” usually means **mini-batch** (small subset of the dataset used each step).\n* Full batch = whole dataset at once (rare in LLM training because it’s too big).\n\n\n\n## 6. **Step (iteration)**\n\n* **Definition**: One update of the model’s weights (using 1 batch).\n* Example: 1 step = take batch → compute predictions → compute loss → backpropagate → update weights.\n* **Analogy**: Like practicing one set of 8 flashcards before moving on.\n\n\n\n## 7. **Epoch**\n\n* **Definition**: One full pass through the **entire dataset** (all sequences).\n* Example: If your 20,000-token book becomes 77 sequences of length 512:\n\n  * Epoch = the model has trained on all 77 sequences once (in shuffled order).\n* **Analogy**: Like rereading your entire book once, in random snippets.\n\n\n\n## 8. **Shuffling**\n\n* **Definition**: Randomizing the order of sequences each epoch.\n* Purpose: Prevents the model from memorizing sequence order.\n* **Analogy**: Shuffling flashcards before each study session.\n\n\n\n## 9. **Loss**\n\n* **Definition**: A number measuring how bad the model’s predictions are.\n* In LLMs → **cross-entropy loss** (how far predicted token probabilities are from the true token).\n* **Analogy**: Like a score on your practice test — lower = better.\n\n\n\n## 10. **Perplexity (PPL)**\n\n* **Definition**: Exponential of average loss → measures how “surprised” the model is by the text.\n* Lower PPL = model predicts words more confidently.\n* **Analogy**: If you’re reading and every next word surprises you → high perplexity; if you can guess words easily → low perplexity.\n\n\n\n## 11. **Checkpoint**\n\n* **Definition**: Saved snapshot of the model’s weights during training.\n* Lets you pause/resume or roll back if training crashes.\n* **Analogy**: Like saving your progress in a video game so you don’t restart from page 1 if your computer dies.\n\n\n\n## 12. **Gradient clipping**\n\n* **Definition**: Limiting how large weight updates can get to avoid exploding gradients.\n* **Analogy**: Putting a “speed limit” on how much you can change your notes at once.\n\n\n\n# Quick Recap with Your Book Example\n\n* **Tokens** = words broken into Lego bricks.\n* **Sequence length** = how many Lego bricks the model can look at at once.\n* **Stride** = how much you slide the reading window each time.\n* **Batch** = how many sequences you train on in one go.\n* **Step** = training update on 1 batch.\n* **Epoch** = reading the whole book once (via chunks).\n* **Shuffling** = randomizing flashcards before each read.\n* **Loss/PPL** = how well the model is guessing the next word.\n* **Checkpoints** = save points during training.\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}}]}