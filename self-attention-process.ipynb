{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Self-Attention Process (Steps 1 â†’ 7)\n\nInput embeddings â†’ Each word (e.g., cat, sat, on, mat) is turned into a vector.\n\nLinear projections â†’ Multiply embeddings by \nğ‘Š\nğ‘„\n,\nğ‘Š\nğ¾\n,\nğ‘Š\nğ‘‰\nW\nQ\n\tâ€‹\n\n,W\nK\n\tâ€‹\n\n,W\nV\n\tâ€‹\n\n to get Q, K, V matrices.\n\nDot products (QKáµ€) â†’ Compare Queries with Keys to measure similarity (relevance of one word to another).\n\nScale â†’ Divide by \nğ‘‘\nğ‘˜\nd\nk\n\tâ€‹\n\n\tâ€‹\n\n so numbers donâ€™t get too large.\n\nSoftmax â†’ Convert scores into probabilities (attention weights). Each row = â€œhow much this word attends to every other word.â€\n\nWeighted sum of V â†’ Multiply attention weights with Value vectors. Each word gets a new contextual vector that blends information from others.\n\nOutputs â†’ For all words, we now have updated embeddings that encode meaning + relationships to surrounding words.\n\nğŸ”¹ How This Helps Predict the Next Word\n\nAfter attention, each tokenâ€™s new vector is passed through feed-forward layers, then into the decoder (for language models).\n\nThe last tokenâ€™s vector (e.g., from â€œcat sat onâ€) carries context from all previous tokens.\n\nThis vector is fed into a linear + softmax layer over the vocabulary.\n\nExample:\n\nâ€œmatâ€ â†’ 75%\n\nâ€œdogâ€ â†’ 10%\n\nâ€œcarâ€ â†’ 5%","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}}]}