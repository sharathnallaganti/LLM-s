{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Self-Attention Process (Steps 1 → 7)\n\nInput embeddings → Each word (e.g., cat, sat, on, mat) is turned into a vector.\n\nLinear projections → Multiply embeddings by \n𝑊\n𝑄\n,\n𝑊\n𝐾\n,\n𝑊\n𝑉\nW\nQ\n\t​\n\n,W\nK\n\t​\n\n,W\nV\n\t​\n\n to get Q, K, V matrices.\n\nDot products (QKᵀ) → Compare Queries with Keys to measure similarity (relevance of one word to another).\n\nScale → Divide by \n𝑑\n𝑘\nd\nk\n\t​\n\n\t​\n\n so numbers don’t get too large.\n\nSoftmax → Convert scores into probabilities (attention weights). Each row = “how much this word attends to every other word.”\n\nWeighted sum of V → Multiply attention weights with Value vectors. Each word gets a new contextual vector that blends information from others.\n\nOutputs → For all words, we now have updated embeddings that encode meaning + relationships to surrounding words.\n\n🔹 How This Helps Predict the Next Word\n\nAfter attention, each token’s new vector is passed through feed-forward layers, then into the decoder (for language models).\n\nThe last token’s vector (e.g., from “cat sat on”) carries context from all previous tokens.\n\nThis vector is fed into a linear + softmax layer over the vocabulary.\n\nExample:\n\n“mat” → 75%\n\n“dog” → 10%\n\n“car” → 5%","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}}]}